{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q57iW4kieZbs","outputId":"a62c55cd-0323-47b3-f70d-0ff3acbbd471"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import os\n","os.chdir(\"/content/drive/MyDrive/Diplomatic2/Implementation\")\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3K6FH6fleqOu","outputId":"e5ebd910-2cdb-4aa7-fc33-f9e897caed8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" AGNN_1\t\t\t    elliptic_txs_edgelist.csv   GRL_2.ipynb\n"," AGNN_2\t\t\t    elliptic_txs_features.csv  'GRL_FINAL (4).ipynb'\n"," AGNN_3\t\t\t    GAT_1\t\t        GRL.ipynb\n"," data_balanced_1000.obj     GAT_2\t\t        NoteBooks\n"," data_balanced_500.obj\t    GAT_2_1\t\t        SAGE_1\n"," data.obj\t\t    GAT_3\t\t        SAGE_2\n"," DGI\t\t\t    GAT_FL_1\t\t        SAGE_3\n"," elliptic_txs_classes.csv   GAT_TEST\n"]}]},{"cell_type":"code","source":["#PYTORCH INSTALL\n","# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n","import torch\n","\n","def format_pytorch_version(version):\n","  return version.split('+')[0]\n","\n","TORCH_version = torch.__version__\n","TORCH = format_pytorch_version(TORCH_version)\n","\n","def format_cuda_version(version):\n","  return 'cu' + version.replace('.', '')\n","\n","CUDA_version = torch.version.cuda\n","CUDA = format_cuda_version(CUDA_version)\n","\n","!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n","!pip install torch-geometric"],"metadata":{"id":"Q7GyOXGvexms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle   \n","import torch\n","from torch_geometric.data import Data\n","\n","fileObj = open('data.obj', 'rb')\n","data = pickle.load(fileObj)\n","fileObj.close()\n","print(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmze3_MRe3Jg","outputId":"4ff4d69b-bd9f-4d10-fc1b-591a5fe00cb1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Data(x=[203769, 165], edge_index=[2, 234355], y=[203769], n_id=[203769], train_mask=[203769], val_mask=[203769], test_mask=[203769])\n"]}]},{"cell_type":"code","source":["#USE GPU\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(device)"],"metadata":{"id":"ez0P-tY3jXG_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f500faca-684a-44cc-b1a6-3cc625b45713"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"code","source":["data = data.to(device)\n"],"metadata":{"id":"gEZDdVUme-3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#EDW NA DW ALLES PERIPTWSEIS GIA BATCHES!!!\n","#Epishs na dw th lista me tous geitones\n","from torch_geometric.loader import NeighborLoader\n","\n","train_loader = NeighborLoader(data,num_neighbors=[-1, -1,-1], shuffle=True,batch_size=203769)\n","\n","counter = 0\n","for batch in train_loader:\n","\n","    print(len(batch.n_id))\n","    print(len(batch.y))\n","    print(type(batch))\n","    print(batch.test_mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L6CwazGKoAf4","outputId":"3e58abbf-4a7b-4395-d98e-1cb014858bf3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["203769\n","203769\n","<class 'torch_geometric.data.data.Data'>\n","tensor([False, False,  True,  ..., False, False, False], device='cuda:0')\n"]}]},{"cell_type":"code","source":["#SCRIPT 24\n","from torch_geometric.nn import DeepGraphInfomax, SAGEConv\n","import torch.nn.functional as F\n","\n","class GraphSAGE(torch.nn.Module):\n","  \"\"\"GraphSAGE\"\"\"\n","  def __init__(self):\n","    super().__init__()\n","    \n","    self.sage1 = SAGEConv(165, 128)\n","    self.sage2 = SAGEConv(128, 64)\n","    self.sage3 = SAGEConv(64, 64)\n","    \n","  def forward(self, x, edge_index):\n","\n","      h = self.sage1(x, edge_index)\n","      h = torch.relu(h)\n","      h = F.dropout(h, p=0.5, training=self.training)\n","      h = self.sage2(h, edge_index)\n","      h = torch.relu(h)\n","      h = F.dropout(h, p=0.2, training=self.training)\n","      h = self.sage3(h, edge_index)\n","      h = torch.relu(h)\n","\n","      return h\n","\n","class SAGE_Classifier(torch.nn.Module):\n","  \"\"\"GraphSAGE\"\"\"\n","  def __init__(self,encoder):\n","    super().__init__()\n","\n","    self.encoder = encoder\n","    #self.encoder.requires_grad_(False)\n","    self.linear = torch.nn.Linear(64,2)\n","    self.optimizer = torch.optim.Adam(self.parameters(), lr=0.005, weight_decay=5e-4)\n","    \n","  def forward(self, x, edge_index):\n","\n","      h = self.encoder(x, edge_index)\n","      h = self.linear(h)\n","\n","      return h, F.log_softmax(h, dim=1)"],"metadata":{"id":"B1IX0gQ3gK8E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#SCRIPT 25\n","from torch_geometric.nn import DeepGraphInfomax, GATConv\n","import torch.nn.functional as F\n","\n","class GATmodel(torch.nn.Module):\n","    def __init__(self):\n","        super(GATmodel, self).__init__()\n","        self.hid = 64\n","        self.in_head = 8\n","      \n","        self.conv1 = GATConv(165, self.hid, heads=self.in_head, dropout=0.6)\n","        self.conv = GATConv(self.hid*self.in_head, self.hid, heads=self.in_head,concat=False, dropout=0.6)\n","        #self.conv2 = GATConv(self.hid*self.in_head, self.hid, heads=self.in_head, dropout=0.6)\n","       \n","          \n","    def forward(self,x, edge_index):\n","        \n","        x = F.dropout(x, p=0.6, training=self.training)\n","        x = self.conv1(x, edge_index)\n","        x = F.elu(x)\n","        x = F.dropout(x, p=0.6, training=self.training)\n","        x = self.conv(x, edge_index)\n","        \n","        \n","        return x   \n","\n","\n","class GAT_Classifier(torch.nn.Module):\n","  def __init__(self,encoder):\n","    super().__init__()\n","\n","    self.encoder = encoder\n","    self.encoder.requires_grad_(False)\n","    self.linear = torch.nn.Linear(64,2)\n","    self.optimizer = torch.optim.Adam(self.parameters(), lr=0.005, weight_decay=5e-4)\n","    \n","  def forward(self, x, edge_index):\n","\n","      h = self.encoder(x, edge_index)\n","      h = self.linear(h)\n","\n","      return h, F.log_softmax(h, dim=1)"],"metadata":{"id":"46JjKrg8xplU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#SCRIPT 26\n","from torch_geometric.nn import DeepGraphInfomax, AGNNConv\n","import torch.nn.functional as F\n","\n","class AGNNmodel(torch.nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.hidden = 128\n","\n","        self.lin1 = torch.nn.Linear(165, self.hidden)\n","        self.prop1 = AGNNConv(requires_grad=False)\n","        self.prop2 = AGNNConv(requires_grad=True)\n","\n","\n","    def forward(self, x, edge_index):\n","\n","        x = F.dropout(x, training=self.training)\n","        x = F.relu(self.lin1(x))\n","        x = self.prop1(x, edge_index)\n","        x = self.prop2(x, edge_index)\n","\n","        return x \n","\n","class AGNN_Classifier(torch.nn.Module):\n","  def __init__(self,encoder):\n","    super().__init__()\n","\n","    self.encoder = encoder\n","    self.encoder.requires_grad_(False)\n","    self.linear = torch.nn.Linear(128,2)\n","    self.optimizer = torch.optim.Adam(self.parameters(), lr=0.005, weight_decay=5e-4)\n","    \n","  def forward(self, x, edge_index):\n","\n","      h = self.encoder(x, edge_index)\n","      h = self.linear(h)\n","\n","      return h, F.log_softmax(h, dim=1)        "],"metadata":{"id":"uQLZfnfT_9RG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#DEBUGGING CODE WORKS!!!!\n","model1 = GraphSAGE()\n","print(model1)\n","model2 = SAGE_Classifier(model1)\n","print(model2)\n","best, trai_lo, val_lo = train2(model2,2,False)\n"],"metadata":{"id":"RNDHDfqZlZCv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#DEBUGGING GAT WORKS\n","model3 = GATmodel()\n","print(model3)\n","model4 = GAT_Classifier(model3).to(device)\n","print(model4)\n","best, trai_lo, val_lo = train2(model4,2,False)\n"],"metadata":{"id":"MxV9foCs86M3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#SCRIPT 16\n","#METRICS\n","!pip install torchmetrics\n","from torchmetrics.classification import AUROC\n","from torchmetrics.classification import BinaryPrecision\n","from torchmetrics.classification import BinaryRecall\n","from torchmetrics.classification import BinaryConfusionMatrix\n","from torchmetrics.classification import MulticlassRecall\n","from torchmetrics.classification import MulticlassPrecision\n","from torchmetrics.classification import MulticlassAUROC\n","from torchmetrics.classification import BinaryF1Score\n","from torchmetrics.classification import MulticlassF1Score\n","\n","confmat = BinaryConfusionMatrix().to(device)\n","recall = MulticlassRecall(num_classes=2, average=None).to(device)\n","precision = MulticlassPrecision(num_classes=2, average=None).to(device)\n","aucroc = MulticlassAUROC(task=\"multiclass\", num_classes=2).to(device)\n","metric = BinaryF1Score(average=None).to(device)\n","metric2 = MulticlassF1Score(num_classes=2, average=None).to(device)\n","\n","def conf_matrix(pred_y, y):\n","  #confmat = BinaryConfusionMatrix().to(device)\n","\n","  return confmat(pred_y, y).cpu().detach().numpy()\n","\n","def brecall(pred_y, y):\n","  #recall = BinaryRecall().to(device)\n","  #recall = MulticlassRecall(num_classes=2, average=None).to(device)\n","\n","  return recall(pred_y, y).cpu().detach().numpy()\n","\n","def bprecision(pred_y, y):\n","  #precision = MulticlassPrecision(num_classes=2, average=None).to(device)\n","\n","  return precision(pred_y, y).cpu().detach().numpy()\n","\n","def auc_roc(pred_y, y):\n","  #aucroc = AUROC(task=\"binary\").to(device)\n","  #print('AUC ROC',pred_y[:10])\n","  #print('AUC ROC',y.shape)\n","  #aucroc = MulticlassAUROC(task=\"multiclass\", num_classes=2).to(device)\n","\n","  return aucroc(pred_y, y).cpu().detach().numpy()\n","\n","def f1score(pred_y, y):\n","  #metric = BinaryF1Score(average=None).to(device)\n","\n","  return metric2(pred_y, y).cpu().detach().numpy()\n","\n","\n","def accuracy(pred_y, y):\n","    \n","    return ((pred_y == y).sum() / len(y)).item() "],"metadata":{"id":"JY73l9FsjP4v","colab":{"base_uri":"https://localhost:8080/"},"outputId":"01afd7fc-6015-40e4-cdc6-161c7589d05a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchmetrics\n","  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.4/512.4 KB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.0+cu116)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n","Installing collected packages: torchmetrics\n","Successfully installed torchmetrics-0.11.0\n"]}]},{"cell_type":"code","source":["# SCRIPT 18\n","\n","def train2(model, epochs,yes_no):\n","    weigths = [2.5, 0.625]\n","\n","    if yes_no == True:\n","\n","      wei = torch.FloatTensor(weigths).to(device)\n","      print('WEIGTHS = ',wei)\n","      criterion = torch.nn.CrossEntropyLoss(weight=wei)\n","    else:\n","      criterion = torch.nn.CrossEntropyLoss()\n","\n","    optimizer = model.optimizer\n","    \n","    model.train()\n","\n","    train_loss_total = []\n","    val_loss_total = []\n","    best_model = model\n","    best_val_loss = 1000.0\n","    \n","    for epoch in range(epochs+1):\n","      total_loss = 0\n","      total_val_loss = 0\n","      acc = 0\n","      val_loss = 0\n","      val_acc = 0\n","      auc_roc_train = 0\n","      auc_roc_val = 0 \n","      pre_train = 0 \n","      pre_val = 0\n","      recall_train = 0\n","      recall_val = 0\n","      f1_train = 0\n","      f1_val = 0\n"," \n","      # Train on batches\n","      \n","      optimizer.zero_grad()\n","        \n","      logit, out = model(data.x, data.edge_index)\n","      loss = criterion(logit[data.train_mask], data.y[data.train_mask])\n","      total_loss += float(loss)\n","\n","      acc += accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n","      auc_roc_train += auc_roc(out[data.train_mask], data.y[data.train_mask])\n","      pre_train += bprecision(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n","      recall_train += brecall(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n","      f1_train += f1score(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n","      conf_matrix_train = conf_matrix(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n","      print(conf_matrix_train)\n","      loss.backward()\n","      optimizer.step()\n","\n","        # Validation\n","      with torch.no_grad():  \n","        val_loss = criterion(logit[data.val_mask], data.y[data.val_mask])\n","        total_val_loss += float(val_loss)\n","        print('TRAIN LOSS', total_loss)\n","        print('VALIDATION LOSS', total_val_loss)\n","\n","        val_acc += accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n","        auc_roc_val += auc_roc(out[data.val_mask], data.y[data.val_mask])\n","        pre_val += bprecision(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n","        recall_val += brecall(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n","        f1_val += f1score(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n","        conf_matrix_val = conf_matrix(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n","        print(conf_matrix_val)\n","\n","        print(\"Train accuracy = \", acc, \"Val accuracy = \", val_acc)\n","        print(\"AUC ROC SCORE TRAIN = \",auc_roc_train,\"AUC ROC SCORE VAL = \", auc_roc_val)\n","        print(\"precision SCORE TRAIN = \",pre_train,\"precision SCORE VAL = \", pre_val)\n","        print(\"RECALL SCORE TRAIN = \",recall_train,\"RECALL SCORE VAL = \", recall_val)\n","        print(\"F1 SCORE TRAIN = \",f1_train,\"F1 SCORE VAL = \", f1_val)\n","\n","        if(total_val_loss < best_val_loss):\n","          best_val_loss = total_val_loss\n","          best_model = model\n","          print(\"NEW BEST at epoch \", epoch)\n","        train_loss_total.append(total_loss)\n","        val_loss_total.append(total_val_loss)  \n","\n","    return best_model, train_loss_total, val_loss_total "],"metadata":{"id":"rjrH_PJZi1G8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#SCRIPT 17\n","def train(model,epochs,yes_no):\n","\n","    weigths = [2.5, 0.625]\n","\n","    if yes_no == True:\n","\n","      wei = torch.FloatTensor(weigths).to(device)\n","      print('WEIGTHS = ',wei)\n","      criterion = torch.nn.CrossEntropyLoss(weight=wei)\n","    else:\n","      criterion = torch.nn.CrossEntropyLoss()\n","\n","    #criterion = torch.nn.CrossEntropyLoss()\n","    #criterion2 = torch.nn.NLLLoss()\n","    optimizer = model.optimizer\n","\n","    train_loss_total = []\n","    val_loss_total = []\n","    best_model = model\n","    best_val_loss = 1000.0\n","\n","    model.train()\n","    for epoch in range(epochs+1):\n","      total_loss = 0\n","      total_val_loss = 0\n","      acc = 0\n","      val_loss = 0\n","      val_acc = 0\n","      auc_roc_train = 0\n","      auc_roc_val = 0 \n","      pre_train = 0 \n","      pre_val = 0\n","      recall_train = 0\n","      recall_val = 0\n","      f1_train = 0\n","      f1_val = 0\n","      nll_loss_train = 0\n","      nll_loss_val = 0\n"," \n","      # Train on batches\n","      for batch in train_loader:\n","        optimizer.zero_grad()\n","        \n","        logit, out = model(batch.x, batch.edge_index)\n","        loss = criterion(logit[batch.train_mask], batch.y[batch.train_mask])\n","        total_loss += float(loss)\n","\n","        #nll_loss_train = criterion2(out[batch.train_mask], batch.y[batch.train_mask])\n","        #nll_loss_val = criterion2(out[batch.val_mask], batch.y[batch.val_mask])\n","\n","\n","        acc += accuracy(out[batch.train_mask].argmax(dim=1), batch.y[batch.train_mask])\n","        auc_roc_train += auc_roc(out[batch.train_mask], batch.y[batch.train_mask])\n","        pre_train += bprecision(out[batch.train_mask].argmax(dim=1), batch.y[batch.train_mask])\n","        recall_train += brecall(out[batch.train_mask].argmax(dim=1), batch.y[batch.train_mask])\n","        f1_train += f1score(out[batch.train_mask].argmax(dim=1), batch.y[batch.train_mask])\n","        conf_matrix_train = conf_matrix(out[batch.train_mask].argmax(dim=1), batch.y[batch.train_mask])\n","\n","        print(conf_matrix_train)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Validation\n","        with torch.no_grad():\n","          val_loss = criterion(logit[batch.val_mask], batch.y[batch.val_mask])\n","          total_val_loss += float(val_loss)\n","\n","          val_acc += accuracy(out[batch.val_mask].argmax(dim=1), batch.y[batch.val_mask])\n","          auc_roc_val += auc_roc(out[batch.val_mask], batch.y[batch.val_mask])\n","          pre_val += bprecision(out[batch.val_mask].argmax(dim=1), batch.y[batch.val_mask])\n","          recall_val += brecall(out[batch.val_mask].argmax(dim=1), batch.y[batch.val_mask])\n","          f1_val += f1score(out[batch.train_mask].argmax(dim=1), batch.y[batch.train_mask])\n","          conf_matrix_val = conf_matrix(out[batch.val_mask].argmax(dim=1), batch.y[batch.val_mask])\n","\n","          #print(conf_matrix_val)\n","          print('TRAIN LOSS', total_loss)\n","          print('VALIDATION LOSS', total_val_loss)\n","\n","          print(\"Train accuracy = \", acc/len(train_loader), \"Val accuracy = \", val_acc/len(train_loader))\n","          print(\"AUC ROC SCORE TRAIN = \",auc_roc_train/len(train_loader),\"AUC ROC SCORE VAL = \", auc_roc_val/len(train_loader))\n","          print(\"precision SCORE TRAIN = \",pre_train/len(train_loader),\"precision SCORE VAL = \", pre_val/len(train_loader))\n","          print(\"RECALL SCORE TRAIN = \",recall_train/len(train_loader),\"RECALL SCORE VAL = \", recall_val/len(train_loader))\n","          print(\"F1 SCORE TRAIN = \",f1_train/len(train_loader),\"F1 SCORE VAL = \", f1_val/len(train_loader))\n","\n","        if(total_val_loss < best_val_loss):\n","          best_val_loss = total_val_loss\n","          best_model = model\n","          print(\"NEW BEST at epoch \", epoch)\n","        train_loss_total.append(total_loss)\n","        val_loss_total.append(total_val_loss)\n","      \n","    return best_model, train_loss_total, val_loss_total\n","\n","def test(model, data):\n","    \"\"\"Evaluate the model on test set and print the accuracy score.\"\"\"\n","    with torch.no_grad():\n","      model.eval()\n","      _, out = model(data.x, data.edge_index)\n","      acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n","      auc_roc_acc = auc_roc(out[data.test_mask], data.y[data.test_mask])\n","      precision_score = bprecision(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n","      recall_score = brecall(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n","      f1_test = f1score(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n","      conf_mat = conf_matrix(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n","\n","\n","    return acc, auc_roc_acc, precision_score, recall_score,f1_test, conf_mat\n"],"metadata":{"id":"EmFlKc3NoG-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#SCRIPT27\n","#TRAIN ENCODER WITH DGI\n","#encoder_SAGE = GraphSAGE()\n","#print(encoder_SAGE)\n","#encoder_GAT = GATmodel()\n","#print(encoder_GAT)\n","#encoder_AGNN = AGNNmodel()\n","#print(encoder_AGNN)\n","\n","def corruption(x, edge_index):\n","    return x[torch.randperm(x.size(0))], edge_index\n","\n","model = DeepGraphInfomax(hidden_channels=128, encoder=AGNNmodel(),summary=lambda z, *args, **kwargs: torch.sigmoid(z.mean(dim=0)),corruption=corruption).to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","def train_dgi():\n","    model.train()\n","    optimizer.zero_grad()\n","    pos_z, neg_z, summary = model(data.x, data.edge_index)\n","    loss = model.loss(pos_z, neg_z, summary)\n","    loss.backward()\n","    optimizer.step()\n","    return loss.item()\n"],"metadata":{"id":"n6LdvSrYpRLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(model.encoder)"],"metadata":{"id":"QR-LYJNw_ER1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(1,200):\n","  loss = train_dgi()\n","  print(loss)"],"metadata":{"id":"epeQ0bpdpldM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#BUILT IN CLASSIFIER\n","model.eval()\n","z, _, _ = model(data.x, data.edge_index)\n","acc = model.test(z[data.train_mask], data.y[data.train_mask], z[data.test_mask], data.y[data.test_mask], max_iter=150)\n","print(acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SAqCsiW0w755","outputId":"aaecb333-09cb-4452-9e4c-59cc19269cb0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.9176300578034682\n"]}]},{"cell_type":"code","source":["#TRAIN AGNN CLASSIFIER\n","#TRAIN SAGE CLASSIFIER\n","AGNN_Class = AGNN_Classifier(model.encoder).to(device)\n","print(AGNN_Class)\n","best, trai_lo, val_lo = train2(AGNN_Class,200,False)\n","acc_SAGE, auc_roc_acc_SAGE, precision_score_SAGE, recall_score_SAGE, f1_test_SAGE, conf_mat_SAGE = test(best,data)"],"metadata":{"id":"wdHJwhG_FEyt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TRAIN GAT CLASSIFIER\n","GAT_Class = GAT_Classifier(model.encoder).to(device)\n","print(GAT_Class)\n","best, trai_lo, val_lo = train2(GAT_Class,200,False)\n","acc_GAT, auc_roc_acc_GAT, precision_score_GAT, recall_score_GAT, f1_test_GAT, conf_mat_GAT = test(best,data)"],"metadata":{"id":"-smaICA0BBvi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#TRAIN SAGE CLASSIFIER\n","SAGE_Class = SAGE_Classifier(model.encoder).to(device)\n","print(SAGE_Class)\n","best, trai_lo, val_lo = train(SAGE_Class,200,False)\n","acc_SAGE, auc_roc_acc_SAGE, precision_score_SAGE, recall_score_SAGE, f1_test_SAGE, conf_mat_SAGE = test(best,data)"],"metadata":{"id":"C77WcNSBojRb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/Diplomatic2/Implementation/DGI\")\n","model = 'AGNN_Frozen.model'\n","#model = 'SAGE_Frozen.model'\n","file2save_everything = open(model,'wb')\n","#pickle.dump(best_model, file2save_everything)\n","pickle.dump(trai_lo, file2save_everything)\n","pickle.dump(val_lo, file2save_everything)\n","pickle.dump(acc_SAGE, file2save_everything)\n","pickle.dump(auc_roc_acc_SAGE, file2save_everything)\n","pickle.dump(precision_score_SAGE, file2save_everything)\n","pickle.dump(recall_score_SAGE, file2save_everything)\n","pickle.dump(f1_test_SAGE, file2save_everything)\n","pickle.dump(conf_mat_SAGE, file2save_everything)\n","\n","file2save_everything.close()"],"metadata":{"id":"sBdRgCRW6UKG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#EVAL CLASSIFIER\n","acc, auc_roc_acc, precision, recall_score, f1_test, conf_mat = test(best,data)\n","print(acc, auc_roc_acc, precision, recall_score, f1_test)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c9EtioMtpfyn","outputId":"553ddd71-e2d8-49ae-8d65-6125bda1dc62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.7574508190155029 0.9358834 [0.24971001 0.9913998 ] [0.93045187 0.7414514 ] [0.39374793 0.84839916]\n"]}]},{"cell_type":"code","source":["#SAVE EVERYTHING FOR COMPARISON\n","import pickle\n","\n","os.chdir(\"/content/drive/MyDrive/Diplomatic2/Implementation\")\n","\n","a = os.path.isdir('DGI')\n","if a == False:\n","  os.mkdir(\"DGI\")\n","\n","os.chdir(\"/content/drive/MyDrive/Diplomatic2/Implementation/DGI\")\n","\n","architecture = 'AGNN.model'\n","\n","file2save_everything = open(architecture,'wb')\n","#pickle.dump(best_model, file2save_everything)\n","pickle.dump(trai_lo, file2save_everything)\n","pickle.dump(val_lo, file2save_everything)\n","pickle.dump(acc, file2save_everything)\n","pickle.dump(auc_roc_acc, file2save_everything)\n","pickle.dump(precision, file2save_everything)\n","pickle.dump(recall_score, file2save_everything)\n","pickle.dump(f1_test, file2save_everything)\n","pickle.dump(conf_mat, file2save_everything)\n","\n","file2save_everything.close()"],"metadata":{"id":"WFnJ-K1eqF57"},"execution_count":null,"outputs":[]}]}